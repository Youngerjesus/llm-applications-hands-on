### 단어를 임베딩하는 방식에 대해서 설명해봐라 

대표적으로 알려진 기법으로 word2vec 이 있음. 

단어가 있다고 한다면 그 단어와 자주 등장하는 단어들이 있을거임. 이렇게 중심 단어를 가지고 주변 단어를 예측하는 방식으로 학습하는 방식인 Skip-gram 과 주변 단어들로부터 중심 단어를 예측하는 기법인 CBOW(Continuous bab of words) 를 통해서 훈련을 하는 것으로 의미를 가진 임베딩을 만들어낼 수 있음. 


### 문장을 임베딩하는 방식에 대해서 설명해봐라 

문장을 의미를 가진 임베딩으로 만드는 방식은 Transformer 아키텍처의 BERT 를 사용하면 얻을 수 있음. 

BERT 모델 자체가 양방향 인코더를 이용해서 문장을 이해하고 이를 고차원 벡터 표현으로 내놓을 수 있음. 

대표적인 모델로 Sentence-BERT 모델이 있다. 

Sentence-BERT 모델은 BERT 기반으로 문장 임베딩 품질을 높히기 위해 문장쌍 간 유사도 학습 과정을 추가로 거친 모델임. 

비교하고 싶은 두 문장을 BERT 모델에게 각각 넣어주고 코사인 유사도를 통해서 문장을 비교하는 것도 가능함. 이걸 바이 인코더 방식이라고 한다. 

또 다른 방식은 한번에 한 쌍의 문장을 BERT 모델류에게 넣고 직접 유사도 비교 점수를 얻는 방식인데 별도의 출력 헤드(분류나 회귀)를 얹은 모델에서 가능하다. 
- \[CLS] 문장 A \[SEP] 문장 B \[SEP] ㅇ이렇게 입력이 되는거임. 
- 분류용 헤드는 유사/비유사를 예측하게 될거고, 회귀 헤드는 유사도 점수를 직접 예측할거임. 
- 학습 방식은 문장 쌍 간의 유사도 점수를 가지고 학습을 하는거다. 

이런 방식은 크로스 인코더 방식인데 바이 인코더 방식보다 더 정교한 유사도 비교를 할 수 있다는 장점이 있다. 대신 직접 비교를 해봐야하니 확장성이 떨어지는 문제가 있지만 

그래서 유사도 비교를 더 정교하게 해보고 싶다면 바이 인코더 방식으로 먼저 1차로 크게 거른 후 크로스 인코더 방식으로 직접 비교하는 방식을 적용하는 방법이 있음. 


### 바이 인코더의 풀링 모드(Pooling mode) 란? 

문장 인코더의 경우 다른 딥러닝 머신러닝 모델의 입력으로 사용하기 위해 고정된 차원의 벡터로 내뱉어야 함. 

이를 위해 고정 길이로 압축하는 Pooling 모드라는게 있음. 

주요 풀링 모드는 다음과 같다: 
- CLS 토큰 풀링: 
    - CLS 위치에 나오는 벡터를 그대로 사용함. 
    - 일반적으로 트랜스포머 아키텍처에서 CLS 는 다음 토큰을 예측하는 작업이었음. 문장 인코더에서의 CLS 두 문장의 관계를 판별하는 분류기와 연결되서 나오는 벡터를 말할거임. 그래서 문장의 의미가 담긴 벡터가 될 것. 
    - 문장 전체 정보를 단일 토큰들에 의존한다는게 단점이지만 구현이 간단함. 
    - 사전 학습에서는 다음 토큰 예측을 가지고 헀으므로 본래 모델이 문장 의미 유사도를 가지고 학습한 건 아님. 

- 평균 풀링: 
    - 모든 토큰의 임베딩 벡터를 더한 후 평균을 내는 방법 
    - 문장 내 모든 토큰 정보를 균등하게 반영한다는 게 장점, 반대로 정보의 중요도 구분 없이 동등하게 처리하기도 함 
    - CLS 풀링 보다 더 안정적이라고 함. 

- 최대 풀링: 
    - 각 차원별로 토큰 임베딩 값 중 가장 최댓값을 선택하는 거. 특징이 가장 강한 걸 찾는거임. 

- 가중치 풀링: 
    - 학습된 가중치를 각 토큰 임베딩에 곱해 합산하는 것. 
    - 중요한 토큰 정보를 비중을 높이는 거임. 추가 학습이 필요하긴 함. 


