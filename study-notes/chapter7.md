### GPU 메모리를 고려한 최적의 배치 크기 개념에 대해서 설명 

GPU에서는 HBM으로부터 모델 파라미터를 온칩 캐시(레지스터/공유메모리)로 옮기는 과정과, 해당 파라미터로 행렬 연산을 처리하는 두 단계가 병렬처럼 겹쳐 돌아갑니다. 이 두 단계 중 어느 하나라도 느리면 전체 파이프라인이 대기하므로, 배치 크기를 조절해 “메모리 대역폭 한계”와 “연산 처리 한계” 사이의 균형점을 찾아야 합니다.

- 배치가 작으면 데이터 이동이 느려 메모리 바운드에 빠지고,
- 배치가 크면 연산량이 많아 컴퓨트 바운드에 빠집니다.

적정 배치 크기는 프로파일링 도구로 메모리 활용률과 SM(utilization)을 함께 보며 선정합니다.
- 너무 큰 배치는 메모리 할당 실패같ㅇ느 것도 일으킬 수 있음. 


최적의 배치 크기 구하는 공식은 다음과 같음: 

- 2 x P x 배치크기 / 하드웨어 연산 속도 = P / 메모리 대역폭 
- 배치 크기 = 하드웨어 연산 속도 / (2 x 메모리 대역폭) 

### GPU 구조는 프로세서 옆에 붙은 SRAM 과 고대역폭 메모리인 HBM 이 있음. LLM 에서 GPU 연산을 할 땐 SRAM 에 파라미터를 옮겨야 함? 아니면 HBM 에 저장된 상태로 바로 연산이 가능한거임? 

ㄴㄴ 

GPU는 외부의 고대역폭 메모리(HBM)에 저장된 데이터를 바로 연산 유닛(ALU)에서 쓰는 것이 아니라, 반드시 온칩(on-chip) 메모리 계층을 거쳐야 함. 


### KV 캐시는 어떻게 GPU 사용량을 줄여줄 수 있는가? 

중복 연산을 줄이는게 KV 캐시의 목적임. 

언어 모델에 프롬프트를 입력하고 다음 토큰을 예측시키는 과정에서 언어 모델은 최종적인 토큰까지 내뱉기까지 앞의 프롬프트는 반복적인 연산을 함. 

예시로 "검은 고양이" 라는 프롬프트를 주고 다음 토큰을 예측시키면 계속해서 "검은 고양이" 는 포함되서 다음 토큰이 예측됨. 물론 예측한 토큰도 다음번부터 반복해서 들어가게 된다. 

이런 연산은 병렬적으로 처리할 수 있어서 빠른편이지만 중복이 많기 때문에 KV 캐시를 통해서 반복을 줄일 수 있음. 

이런 특성 때문에 KV 캐시는 추론 모드에서 사용됨 


### KV 캐시가 사용하는 메모리 양은 어떻게 예측할 수 있을까? 

KV 캐시 메모리 = 2바이트 x 2(키와 값) x 레이어 수 x 토큰 임베딩 차원 x 최대 시퀀스 길이 x 배치 크기 

- 2바이트는 FP16 기준으로 사용한다는 전제임 
- 2 는 어텐션 메커니즘에서 키와 값을 나타냄 
- 레이어 수는 각 레이어마다 KV 캐시를 유지하기 때문임. 
- 최대 시퀀스 길이는 이 길이 만큼 차지할 수 있기 떄문에 확보하기 위함임. 
- 배치 크기가 늘어나면 그만큼 한번에 처리하는 KV 캐시가 늘어날거임. 

예시로 llama-2-13b 모델을 가지고 계산해보자. 

레이어 수: 40 
토큰 임베딩 차원: 5120 
최대 시퀀스 길이: 4096

계산식에 따르면 배치 1당 필요한 KV 캐시는 3.1GB 정도 됨. 

40GB GPU 를 쓴다면 모델만 26GB 차지하기 때문에 최대 4개의 배치 크기만 사용이 가능할 것. 

(이후에 소개하지만 이건 최적의 배치 크기가 아님. 그래서 GPU 를 최대로 활용하지 못할 것)


### 멀티 쿼리 어텐션과 그룹 쿼리 어텐션에 대해서 설명 

멀티 헤드 어텐션에 대해서 먼저 설명해야함. 

멀티 헤드 어텐션은 헤드의 개수만큼의 키와 값 벡터가 쿼리 벡터와 계산하기 때문에 여러 관점을 반영하게 됨. 

하지만 이런 멀티 헤드 어텐션 매커니즘은 여러 키와 값 벡터를 사용하기 때문에 차지하는 KV 캐시 메모리 양이 상당함. 

이러한 문제를 해결하기 위해 멀티 쿼리 어텐션이 등장함. 여러개의 쿼리 벡터에 하나의 키와 값 벡터를 공유하는거임. 그렇기 때문에 성능이 


### 양자화 방식에 대해서 설명 (비츠앤바이츠(BitsAndBytes) 와 GPTQ(GPT Quantinization) AWQ(Activation-aware Weight Quantization)) 

비츠앤바이츠의 대표적인 기법은 8비트 행렬 양자화임. 

이거는 8비트로 양자화 할 때 이상치와 같은 큰 값은 중요한 정보를 포함하고 있다는 전제하에 16비트로 그대로 유지하는 기법이다. 

그래서 양자화를 할 때 8비트로 줄이는 부분과 16비트로 그대로 두는 부분을 분리해서 양자화를 하고 계산을 함. 

8비트로 된 부분은 실제로 계산될 때 dequantize 되서 (스케일 값을 곱함) FP16으로 되고 이후 저장된 FP16 Outlier 값을 그대로 더해줘서 16비트로 되는거임. 

메모리에는 8비트로 압축되서 올라가지만 실제 추론시에는 16비트가 됨. 

GPTQ 양자화 기법은 양자화 전과 후의 차이가 없으면 되는거 아니냐는 물음에 답하는 기법임. 

양자화를 위한 특별한 데이터 셋을 준비하고 양자화 전과 후의 계산 결과값의 차이가 안나도록 근사화 시키는 거임  

AWQ 기법은 중요한 파라미터의 경우에는 최대한 그대로 두고, 나머지 파라미터는 양자화를 하는 기법임. 중요한 파라미터는 값이 크거나 활성화 되는 빈도가 많은 파라미터를 말함. 

근데 중요한 파라미터만 그대로 16비트로 둔다거나 그러면 하드웨어 연산의 효율성을 최대로 사용할 수 없다는 문제가 있다. 

그래서 양자화를 하면서도 중요한 파라미터의 정보를 최대로 유지하는 방법이 필요한데 AWQ 에서는 이를 스케일러를 곱하는 것으로 해결함. 

가장 대표적인 스케일 값인 2를 파라미터에 곱해서 값을 크게 만들면 양자화 할 때 값의 손실이 적어지기 때문에 이 값을 곱하는거임. 

2보다 큰 값을 곱하는 경우에는 파라미터 값이 너무 커져서 그 값도 다른 파라미터의 양자화에 영향을 주기 때문에 너무 큰 스케일 값을 곱하는 건 권장되지는 않음. 

(물론 스케일은 채널별로 그룹별 계산된 최적의 값을 곱하긴 함.)