### RLHF 의 한계점은 뭐임? 

보상 모델에는 여러가지 한계점이 있음. 

먼저 RM 이 SFT 모델보다 훨씬 작을 경우 보상 추론력이 떨어지는 경우도 생길 수 있음. (규모가 차이나면 표현력과 추론력에 차이가 나기 떄문)

이러한 허숧한 RM 은 보상 해킹등의 문제를 일으켜서 실속없는 답변을 만들어낼 수도 있음. 

이러한 문제를 해결하기 위해서는 고품질의 데이터셋 구축, RM 모델의 사이즈를 키워야하는데 이것 자체가 큰 비용을 유발하기도 함.  

그리고 강화학습 자체가 하이퍼파라미터에 굉장히 민감하다고 함. 


### DPO 는 선호 데이터를 가지고 학습에 사용하는거임? 

맞다. (프롬프트, 선택된 응답, 거절된 응답) 을 가지고 분류 손실을 계산해서 정책을 업데이트하는거임. 


