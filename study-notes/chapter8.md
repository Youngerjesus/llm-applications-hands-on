### 언어 모델에는 입력을 배치로 전달하고 배치로 생성한다. 이때 하나의 문장은 토큰 생성이 끝났지만 배치 내의 또 다른 문장의 생성 완료를 기다리는 경우가 있는데 이때 지연 시간이 증가하는 문제를 낳는다. 이 문제를 해결하는 최적의 배치 전략 기법은? 

일단 배치 기법들에 대해서 보자. 질문에서 소개한 기법은 정적 배치임. 배치 내의 모든 문장의 처리가 끝나야 다음 배치를 처리하기 때문에 지연시간이 길다는 단점이 있음. 

그 다음으로 동적 배치가 있음. 동적 배치는 비슷한 시간대에 온 요청을 배치로 묶어서 전달하는 방법임. 

큐를 이용해 일정 수준(request count 또는 최대 대기 시간 기준) 으로 배치를 자동으로 구성함. 이렇게 시작하는 것만으로도 약간의 지역시간을 줄일 수 있지만 정적 배치의 근본 문제를 해결하지는 못함..

그 다음 연속 배치가 있는데 이건 토큰 생성이 완료된 문장을 제거하고 대기중인 문장을 배치로 넣어서 이어서 처리하는 방법임. 

무턱대고 처리가 끝나자마자 대기중인 문장을 넣는건 아니고 대기중인 문장과 처리중인 문장의 비율을 보고 결정한다. 

대표적인 라이브러리로는 Text-Generation-Inference 라이브러리가 있음. 

### 어텐션 연산을 효율적으로 할 수 있는 플래시어텐션(flash attention) 은? 

어텐션 연산에서 실제로 시간이 많이 걸리는 연산은 행렬곱, 마스크, 소프트맥스, 드롭아웃이 있다. 
- Q * KT 는 어테텐션 스코어 계산을 말함 
- 마스크는 불필요하거나 금지된 위치에 대한 스코어를 -inf 로 바꾸는 것을 말함 

행렬곱이 가장 많은 시간이 소요될 것 같지만 오히려 다른 연산들이 많은 시간이 걸림. 그 이유는 GPU 내의 큰 메모리를 SRAM 으로 옮기는 연산이기 때문임. 

실제로 GPU 연산을 하는 것보다 메모리를 옮기는 연산이 시간이 많이 소요되는 것. 

플래시 어텐션은 이러한 문제를 해결하기 위해 큰 행렬 메모리를 모두 메모리에 올리고 내리고 하는게 아니라 블록 단위(B x B)로 메모리를 관리해서 필요한 메모리만 옮기는 기법으로 해결함. 

이 방식을 사용하면 N x N 의 행렬을 모두 메모리에 저장하지 않아도 된다는 장점이 있지만 역전파 계산에서는 이 값이 필요하다. 
- 기본 어텐션 매커니즘에서는 N x N 크기의 스코어 매트릭스를 메모리에 그대로 저장해두기 때문에 재계산없이 미분할 수 있다. 


이런 문제를 해결하기 위해서 플래시 어텐션은 순전파를 다시 진행해서 계산은 더 많이 하지만 총 소요 시간은 줄어든다. 

- 이렇게 순전파에서 진행을 할 때도 필요한 메모리만 불러오면 되므로 시간이 짧음. 큰 메모리를 계산하고 가지고 있는게 아니라 필요한 계산만 하고 폐기하는 것. 


### 플래시 어텐션 기법의 속도를 한단계 더 향상 시킨 플래시 어텐션 2는? 

플래시 어텐션의 단점은 GPU 를 최대로 활용하지 못하는거임. 아무래도 블록 단위로 처리하다 보니까 순전파나 역전파에서 GPU 활용량이 떨어짐. 

이런 문제를 해결하기 위해 플래시 어텐션 2가 나온 건데 다음과 같은 기법을 이용: 
- 행렬 곱셈이 아닌 연산을 줄임
    - 커널 퓨전 기술을 통해서 단일 커널에서 연산을 합쳐서 줄임. 
    - 기존에는 마스크 적용, softmax 계산, 스케일링, dropout 적용 등을 단일 연산으로 취급하고 중간 계산을 메모리에 썼다면 이제는 하나의 연산으로 합쳐진 것. 
- 시퀀스 길이 방향으로 잘라서 병렬 처리를 함. (시퀀스 샤딩(Sequence Parallelism))
    - 시퀀스 길이 차원으로 타일을 자름. X 축으로는 쿼리 불록, Y 축으로는 키 블록, Z 축으로는 시퀀스를 분할함. 

GPU 는 행렬 연산과 비행렬 연산의 처리 속도는 약 16배 정도 차이가 남. 플래시 어텐션 2 에서는 이런 비행렬적인 연산을 줄였다고 한다. 

그리고 GPU 의 병렬 처리 단위는 스레드 블록 (처리 단위인 스레드가 32개 정도 모이고, 이를 한번에 처리하는 단위인 warp 가 있는데 이런 warp 가 4~8개 정도 모이면 스레드 블록이 됨) 인데 배치 크기가 작다면 병렬 처리하기 어려운 문제가 생김. 

그래서 시퀀스 길이 방향으로 잘라서 병렬 단위를 늘려서 GPU 를 더 잘 활용하는 방법을 플래시 어텐션 2는 사용함.


### KV 캐시를 효율적으로 관리해서 배치 크기를 증가시킬 수 있는 페이지어텐션(Paged Attention) 은? 

기존 KV 캐시를 사용했을 때의 문제점은 얼만큼의 토큰을 생성할 지 몰라서 최대 토큰 시퀀스 길이만큼 메모리를 예약한다는 것이다. 

즉 불필요하게 예약되는 메모리가 많았다는 문제점이 있었음. 

페이지 어텐션은 가상 메모리를 참고한 기법으로 최대 토큰 시퀀스 길이만큼 메모리를 예약하지 않고 지정된 블록 크기 만큼의 메모리를 예약한다. 

그리고 이러한 예약된 메모리도 논리적 메모리와 물리적 메모리로 관리하고 매핑 테이블도 있기 때문에 한번에 많은 메모리를 에약하지 않고 필요한 만큼만 쓸 수 있다. 

이러한 기법은 병렬 샘플링 (= 하나의 프롬프트에서 여러개의 출력을 샘플링 하는 것)에서 큰 장점을 발휘함. 

동일한 프롬프트는 같은 메모리 공간을 쓰고 달라지기 시작하면 그때 참조 카운트를 하나 늘리고 copy-on-write 해서 복사하는 식으로 메모리를 효율적으로 쓰는 방법으로 사용함. 


### GPU 에서 연산의 효율성을 높히기 위한 커널 퓨전(Kernel fusion) 은? 

반복적으로 수행하는 연산을 하나로 합치는 기법을 말함. 

대표적으로 반복적으로 수행하는 건 HBM 에서 메모리를 읽어오고 결과를 쓰는 것임. 매번 읽어오도록 하지말고 한번에 읽고 처리하도록 하는 기법을 말한다. 
-  HBM↔Shared Memory 뿐 아니라 Shared Memory↔Register나 Register↔Register 왕복도 줄일 수 있음 

어텐션 매커니즘에서는 softmax, scale, dropout, bias-add 같은 후처리 연산들이 반복되기도 하는데 이것들을 매번 메모리에서 읽고 쓰는 것보다 한번에 하는 것이 더 효율적일거임. 
- 메모리 접근 횟수와 커널 호출 횟수가 크게 줄어든다. 


### 대표적인 LLM 서빙 프레임워크인 vLLM 이란? 

vLLM은 대규모 언어 모델(LLM)의 추론(inference)과 서빙(serving)을 위해 설계된 고성능·메모리 효율 라이브러리. 

그냥 huggingface 로 모델을 로드해서 사용하는 것보다 훨씬 나음. 

주요 특징
1.	PagedAttention 기반 메모리 관리
- KV 캐시를 가상 메모리(Page)처럼 블록 단위로 할당·공유·복제하여 “실제 사용된 만큼만” 물리 메모리를 소비
- 최대 시퀀스 길이 전체를 미리 예약하지 않고, 페이지 테이블로 필요한 블록만 매핑 → 메모리 낭비 ‘제로’에 근접  ￼

2.	Continuous Batching (연속 배치)
- 완료된 요청 슬롯을 즉시 대기 큐의 새 요청으로 채워 GPU 활용률을 극대화
- 전통적 정적 배치 대비 평균·퍼센타일 지연(latency)을 크게 줄여 줌  ￼

3.	최적화된 CUDA 커널
- FlashAttention, FlashAttention 2와 유사한 커널 퓨전 및 파이프라인 오버랩 기법으로 비행렬 연산 비중을 최소화
- softmax·scale·dropout·bias-add 같은 후처리 연산을 하나의 fused 커널로 수행해 메모리 왕복을 줄임  ￼

4.	유연한 API & 확장성
- Hugging Face Transformers 호환, Triton Inference Server 백엔드 지원(vllm_backend)
- Ascend NPU, AWS Inferentia 등 다양한 하드웨어 플러그인 제공
- OpenAI 호환 REST API로 손쉬운 통합


