### GPU 연산 장치는 어떤 일을 하는가? 

딥러닝 모델은 행렬 곱셈을 엄청 많이하고 답을 내놓는데, GPU 는 이러한 행렬 곱셈과 덧셈을 아주 많이 할 수 있는 장치임 


### GPU 를 사용할 때 메모리를 효율적으로 활용할 수 있는 기술은? 

그레디언트 체크포인트(Gradient Checkpoint) 와 그레디언트 누적(Gradient Accumulation) 이 있음. 


### 딥러닝 모델에서 표현할 수 있는 수인 fp32, fp16, bf16 은? 

fp32 는 지수 8, 가수 23비트, fp16 은 지수 5, 가수 10, bf16 은 지수 8 가수 7임. 

bf16 이 등장한 이유는 fp16 이 표현할 수 있는 수의 범위가 적기 때문임. 

(추가로 파라미터 10억개 기준 fp16 으로 되어있다면 약 2GB 메모리를 차지한다고 생각하면 됨)

### 양자화 기술의 핵심은? 

더 적은 비트의 숫자로 표현을 하는 거니까 최대한 이 비트의 숫자를 낭비없이 활용하는 게 중요.

가장 간단한 양자화 기술은 최솟값과 최대값을 각각 대응시키고 그 대응에 맞게 나머지 숫자도 변환하는 거임. 

이 방법은 데이터가 균일하게 분포되지 않았을 경우에 낭비되거나 중복되는 값이 많이 생길거임. 

또 다른 방법은 존재하는 데이터의 최대값과 최솟값으로 대응시키는 방법도 있는데 이 방법은 이상치에 취약함. 

꽤 괜찮은 방법은 다음과 같다: 
- 원본 데이터에 랭킹을 매기고 그 랭킹에 맞게끔 데이터의 숫자를 배열하는 것. (그러나 랭킹을 위한 추가적으로 관리해야하는 데이터가 생긴다)
- 정규분포를 따른다는 가정하에 데이터를 변환하는 것. 


### GPU 메모리에 올라가는 데이터들은? 

다음과 같다: 
- 모델 파라미터: 가중치와 편향을 포함한 학습 가능한 텐서  
- 그레디언트: 역전파로 계산된 각 파라미터별 미분값  
- 옵티마이저 상태: 파라미터를 업데이트 하기 위한 추가 정보로 Adam 과 같은 모멘텀 추정치임.  
- 순전파 상태: 순전파 중 각 레이어의 출력값 (역전파 계산시 그레디언트 계산을 위해 저장해야함) 

### GPU 메모리 추정 방법은? 

파라미터 10억개 (fp16) 기준 2GB 

그레디언트는 파라미터와 동일하게 메모리를 차지한다고 생각하면 됨. 

옵티마이저 상태는 파라미터 개수의 2배 정도로 계산하면 됨 

순전파 상태는 모델 구조 베채 크기에 따라 달라지지만 파라미터 메모리 1배를 잡으면 됨 

이후 안전 마진으로 10~20% 정도 추가 메모리가 있으면 된다. 

여기서 추가로 배치 크기를 2배 늘리면 순전파 상태도 2배 는다고 생각하면 됨. 

옵티마이저 종류에 따라 메모리도 달라지는데 일반적으로 2배수로 잡았지만 Adam 을 이용한다면 파라미터 3배수로 잡아야함.


### 그레디언트 누적 기술이란? 

큰 배치 크기를 가진 것과 같은 효과를 내는 기법임. (배치크기가 크면 일반적으로 학습의 수렴이 더 잘함)

구현 방법은 하나의 배치일 때 한번에 바로 파라미터를 업데이트 하는게 아니라 여러 스텝의 그레디언트를 합산해  업데이트를 하는거임. 

일반적으로 한 번의 순전파와 역전파 과정이 끝나면 optimizer.step() 으로 곧바로 파라미터를 업데이트하고 optimizer.zero_grad() 로 그레디언트를 지움. 

하지만 여기서는 여러번의 순전파와 손실 계산 그리고 역전파를 하면서 그레디언트를 누적시키고 이후에 업데이트한다. 


### 그레디언트 누적 기술을 할 때 손실 계산에서 누적 스텝만큼 나누는 이유는?

한번의 그레디언트가 너무 크게 되지 않도록 하기 위함임. 


### 그레디언트 체크포인트 기술은? 

역전파 계산을 위해 순전파 상태를 모두 저장하지 않고 일부만 저장함으로써 메모리 사용량을 줄이는 기술임. 

순전파 상태를 모두 지우면 더 많은 메모리 절약을 하지만 이 경우 추가 계산이 많으므로 균형을 잡아서 일부 순전파 상태만 기억을 하고 필요하면 다시 계산하는 방법을 사용함. 