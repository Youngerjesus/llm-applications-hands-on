### RNN 의 처리 방식 vs 트랜스포머 처리 방식 

정보 처리 방식의 차이: 
- RNN(순환 신경망): 순차적으로 처리합니다. 시퀀스의 각 요소를 하나씩 처리하며 이전 상태의 정보를 현재 계산에 활용합니다.
- 트랜스포머: 병렬적으로 처리합니다. 전체 시퀀스를 한번에 입력받아 모든 요소 간 관계를 동시에 계산합니다.


### 트랜스포머 아키텍처에서 셀프 어텐션 연산은 병렬 연산이 어떻게 동작하는가? 


동시 임베딩 변환: 
- 입력 시퀀스의 모든 토큰 임베딩(X)을 한 번에 행렬 연산으로 처리합니다.
- 단일 행렬 곱셈으로 모든 토큰에 대한 Query(Q), Key(K), Value(V) 행렬을 동시에 생성합니다.


어텐션 스코어 계산: 
- 모든 토큰 쌍 간의 어텐션 스코어를 한 번의 행렬 곱셈으로 계산합니다.



### 트랜스포머 아키텍처의 처리 과정 

인코더 부분: 
- 1. 입력 및 임베딩: 
    - 텍스트 입력을 토큰으로 분리하고, 각 토큰을 고정 크기의 벡터(임베딩)로 변환합니다.
    - 이 임베딩은 단어의 의미적 특성을 밀집된 벡터 공간에 표현합니다.

- 2. 위치 인코딩(Positional Encoding)
    - 트랜스포머는 순환 구조가 없기 때문에 단어의 순서 정보가 필요합니다. (개가 사람을 물었다 vs 사람이 개를 물었다.)
    - 사인과 코사인 함수를 이용한 위치 인코딩을 임베딩에 더해 단어의 상대적 또는 절대적 위치 정보를 포함시킵니다.
    - (사인과 코사인 함수를 이용하는 이유는 일정하게 상대적인 거리를 표현할 수 있기 때문, 문장의 길이가 길어도 일정한 값으로 나타낼 수 있기 떄문임. 절대적인 거리로 나타내면 학습에 어렵기도 함)


- 3. 멀티 헤드 어텐션(Multi-Head Attention)
    - Query(Q), Key(K), Value(V) 세 가지 선형 투영을 통해 입력을 변환합니다.
        - 선형 투영은 가중치 행렬을 이용해서 차원을 나눠서 wx + b 와 같은 새로운 벡터 표현으로 변환하는 방법임. 
        - 여기서 w 는 학습 가능한 행렬 파라미터임. 이를 통해서 문장 내의 각 단어들의 관계를 해석하는 방법을 학습할 수 있는 것. 
    - 어텐션 가중치는 Q와 K의 내적을 통해 계산되고, 이를 소프트맥스로 정규화합니다.
    - 여러 개의 헤드로 나누어 병렬적으로 어텐션을 계산하여 다양한 관점에서 문맥을 파악합니다.
    - 각 헤드의 결과를 연결(concatenate)하고 다시 선형 투영합니다.

- 4. 층 정규화(Layer Normalization)
    - 어텐션 출력에 적용되어 학습을 안정화합니다
    - 입력의 평균과 분산을 이용하여 정규화하고, 학습 가능한 스케일과 편향 매개변수를 적용합니다.

- 5. 잔차 연결(Residual Connection)
    - 층 정규화 전후에 입력을 더해 그래디언트 소실 문제를 완화합니다.


- 6. 피드 포워드 네트워크(Feed-Forward Network)
    - 두 개의 선형 변환과 그 사이의 ReLU 활성화 함수로 구성됩니다.
    - 각 위치에 독립적으로 적용되어 지역적 특징을 추출합니다.
    - 다시 층 정규화와 잔차 연결이 적용됩니다.


디코더 부분: 
- 1. 마스킹된 멀티 헤드 어텐션(Masked Multi-Head Attention)
    - 인코더와 유사하지만 미래 위치를 볼 수 없도록 마스킹을 적용합니다.
    - 자기 자신과 이전 위치의 단어만 참조 가능합니다.

- 2. 크로스 어텐션(Cross-Attention)
    - 디코더의 쿼리(Q)와 인코더 출력의 키(K)와 값(V)을 사용합니다.
    - 이를 통해 디코더가 인코더가 처리한 입력 시퀀스의 정보를 활용할 수 있게 됩니다.
    - 마스킹된 어텐션 이후에 층 정규화와 잔차 연결이 적용됩니다.


- 3. 피드 포워드 네트워크
    - 인코더와 동일한 구조로, 크로스 어텐션 이후에 적용됩니다.
    - 마찬가지로 층 정규화와 잔차 연결이 적용됩니다.

- 4. 최종 출력
    - 마지막 층의 출력에 선형 변환과 소프트맥스를 적용하여 다음 토큰의 확률 분포를 생성합니다.


### 어텐션 매커니즘에 대해 이해한 걸 설명: 

사람이 글을 이해하는 방식을 먼저 생각해보면 단어 개별 개별을 이해하기 보다는 각 단어의 관계와 맥락을 따져서 이해함. 

이런 방식을 이용해서 컴퓨터에게 글을 읽는 방식을 가르켜보고 싶음. 

가장 간단한 방법은 모든 단어의 관계를 평등하게 생각하고 관계를 계산해보는 거임. 하지만 이 방법은 중요한 단어들의 관계 계산을 더 가중치를 줘서 하지 못함. 

다른 방법은 거리 사이의 차등을 줘서 가까운 단어끼리는 더 관계가 있다고 계산하는거임. 이것도 일반적인 경우에는 잘 통할 수 있으나 항상 적용되는 방법은 아님. 

또 다른 방법은 임베딩을 통한 벡터 내적을 통해 관계를 계산해보는거임. 임베딩 벡터의 내적은 의미적 유사도를 가진 경우에서는 해석을 잘하는 반면에 여러 언어적인 관계(반의적, 인과관계 등)은 해석하기 어려움. 

그래서 어텐션 매커니즘에서는 단어 사이의 관계를 파악하는 방법을 학습하기 위해 가중치를 도입해서 이 가중치를 학습시키는 방법을 적용하는거임. 

어텐션 메커니즘에서는 크게 3가지 벡터가 있음. Query(Q), Key(K), Value(V) 

Query 는 현재 초점을 맞춰서 관계를 계산하고 싶은 단어를 말하고, Key 는 모든 단어가 가지는 벡터로 쿼리와 얼마나 관련이 있는지 계산하기 위해 사용됨. 이걸 내적을 통해 계산해서 단어들과 얼마나 관련이 있는지 숫자로 나타내게 됨. 

이후 Value 벡터(실제 정보를 저장하고 있는 벡터) 와 관련도 값을 모두 가중합해서 최종 표현을 나타냄

### 어텐션 메커니즘에서 쿼리와 키 내적값은 보통 Key 벡터 차원의 제곱근으로 나누어 스케일링하는 이유는? 

벡터 차원이 커지면 절대값도 커지고 값의 분산도 커지는 경향이 있음. 

값의 분산이 커지면 값의 격차가 커지게 된다는건데, 이후 소프트맥스 함수를 통해 확률 분포로 변환할 때 이 값들의 격차가 커질 수 있음. 

소프트맥스의 확률 분포 값의 격차가 커지게 되면 큰 값만 유의미하게 업데이트되고, 작은 그레디언트 값은 소실되는 문제가 생길 수 있음. 

즉 그레디언틔 값의 안정성을 이유로 스케일링 하는거임. 


### 트랜스포머 아키텍처에서 층 정규화란 뭐고, 왜 배치 정규화를 사용하지 않을까?

정규화라는건 평균을 빼고 표준편차로 나누는 걸 말함. 

층 정규화를 하는 이유는 레이어간의 출력을 일정한 안정한 값으로 만들어서 학습의 안정성을 높이기 위한거임. 

배치 정규화가 유명하긴한데 언어 모델의 입력 길이는 가변적이라서 일정한 값으로 맞추기 어렵기 때문에 사용하지 않음. 일반적으로 이미지 모델에서 이걸 적용하는 걸로 안다. 


### 피드 포워드 층은 어떤 역할? 

멀티 헤드 어텐션이 다양한 관점에서 단어와의 관계를 계산하는 거라면 피드 포워드 층은 비선형 변환을 통해 언어적인 표현을 가능하게 해주는 역할음 함

- 단순 선형 변환으로는 포착하기 힘든 고차원 언어 현상을 포착 
- 토큰 표현 차원을 일시적으로 크게 늘린 뒤 줄이면서 중요한 차원만 남김 

### 피드 포워드 층의 내부 구성 요소는? 

1) 입력: 

- 셀프 어텐션의 출력을 받아옴 

2) 선형 변황: 

- 입력 표현을 더 큰 차원으로 변환함. 입력 간의 복잡한 상호작용을 포착하기 위해. (고차원 공간에서는 저차원보다 패턴이 분리되어 있다고 함. 그래서 비선형 활성화로 중요 정보를 뽑아내기 용이함.)


3) 비선형 활성화 함후: 

- 복잡한 패턴을 찾아내기 위해 


4) 드롭아웃

- 과적합 방지, 일반화 능력 상승을 위해 


5) 두 번째 선형 변환: 

- 원래 차원으로 축소해서 적절한 크기로 줄이기 위함 

6) 잔차 연결: 

- 깊은 네트워크에서 그레디언트 소실 문제를 완화하기 위해 

7) 층 정규화: 

- 그레디언트 폭발 문제를 줄이기 위해서 