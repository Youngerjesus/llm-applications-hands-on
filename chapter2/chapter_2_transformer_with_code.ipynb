{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
      "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
      "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
      "input_ids:  [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 분리\n",
    "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
    "input_text_list = input_text.split()\n",
    "print(\"input_text_list: \", input_text_list)\n",
    "\n",
    "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
    "str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n",
    "idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n",
    "print(\"str2idx: \", str2idx)\n",
    "print(\"idx2str: \", idx2str)\n",
    "\n",
    "# 토큰을 토큰 아이디로 변환\n",
    "input_ids = [str2idx[word] for word in input_text_list]\n",
    "print(\"input_ids: \", input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 아이디에서 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "\n",
    "# 일반 Python 리스트나 NumPy 배열을 직접 입력할 수 없으며, PyTorch 텐서 형태로 변환해야 함. \n",
    "input_embeddings = embed_layer(torch.tensor(input_ids)) # (5, 16)\n",
    "\n",
    "# unsqueeze(0) 을 통해서 배치 처리를 위한 차원을 추가함. \n",
    "input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16)\n",
    "\n",
    "# shape 를 통해 텐서의 차원을 드러냄 \n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쿼리, 키, 값 벡터를 만드는 nn.Linear 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2674,  0.2305,  0.5756, -0.7030,  0.2837, -0.9053,  0.5753,\n",
       "           0.6657, -1.2195, -0.5005,  0.1554, -0.0896, -0.2492, -0.0015,\n",
       "           0.8065, -0.7903],\n",
       "         [ 0.1401, -0.2307,  0.2017,  0.1211, -0.1198, -0.0447,  0.2522,\n",
       "          -0.5284, -0.2062, -0.5332, -0.3514,  0.3258,  0.7349,  0.0908,\n",
       "          -0.2024, -0.2072],\n",
       "         [ 0.5048, -0.5202,  0.3082,  0.8571, -0.4276,  0.3955,  0.1452,\n",
       "           0.3539, -0.8008, -0.0667,  0.5983,  0.5549, -0.4966, -0.2434,\n",
       "           0.2790, -0.6430],\n",
       "         [-0.6061, -0.6545,  1.3273, -0.3269,  0.0348, -0.6458,  0.4469,\n",
       "          -0.0658, -0.0821, -0.8154, -0.1225, -0.4850,  0.0793,  0.2438,\n",
       "           0.1696, -0.5300],\n",
       "         [-0.5524,  0.4983,  0.1634,  0.5285, -0.4336, -0.2930, -0.0177,\n",
       "          -0.5617,  0.3320,  0.4480, -0.2690, -0.4716,  0.1827, -0.1311,\n",
       "           0.0356, -0.2522]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dim = 16\n",
    "\n",
    "# 쿼리, 키, 값을 계산하기 위한 변환\n",
    "weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "# 변환 수행\n",
    "querys = weight_q(input_embeddings) # (1, 5, 16)\n",
    "keys = weight_k(input_embeddings) # (1, 5, 16)\n",
    "values = weight_v(input_embeddings) # (1, 5, 16)\n",
    "\n",
    "querys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스케일 점곱 방식의 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights:  tensor([[[-4.0728e-02,  6.2884e-02,  8.4439e-02,  4.8278e-01,  1.2792e-01,\n",
      "           3.8908e-01, -1.8602e-01, -1.3477e-01,  3.2630e-01,  3.6521e-01,\n",
      "          -9.8511e-02, -2.5141e-01,  2.5594e-01, -1.5738e-02, -1.0915e-01,\n",
      "          -6.6629e-02],\n",
      "         [-3.1253e-02,  1.4749e-01,  7.8763e-02,  5.1056e-01,  1.1171e-01,\n",
      "           3.5293e-01, -3.4189e-01, -1.2582e-01,  3.2494e-01,  4.1650e-01,\n",
      "           3.2637e-02, -1.8003e-01,  1.6691e-01, -7.4642e-03, -4.7548e-02,\n",
      "          -1.4196e-01],\n",
      "         [-7.1765e-02,  6.9422e-02,  3.1493e-02,  4.8302e-01,  1.3351e-01,\n",
      "           4.1274e-01, -2.1709e-01, -1.3505e-01,  3.2711e-01,  3.9952e-01,\n",
      "          -1.0553e-01, -1.9786e-01,  2.5885e-01, -2.0225e-04, -7.4752e-02,\n",
      "          -1.0750e-01],\n",
      "         [-2.2770e-02,  6.9439e-02,  4.8640e-02,  4.5960e-01,  1.1847e-01,\n",
      "           3.6570e-01, -2.4218e-01, -9.6323e-02,  3.1936e-01,  3.8622e-01,\n",
      "          -3.8909e-02, -2.3431e-01,  2.2183e-01, -1.6116e-02, -6.0861e-02,\n",
      "          -6.0638e-02],\n",
      "         [-6.0040e-03,  1.5032e-01,  9.0612e-02,  5.0606e-01,  1.2197e-01,\n",
      "           2.6327e-01, -4.3201e-01, -7.7865e-02,  2.8073e-01,  3.8166e-01,\n",
      "           1.0104e-01, -2.0795e-01,  9.3661e-02, -2.5681e-02, -1.7291e-02,\n",
      "          -9.5954e-02]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention(querys, keys, values, is_causal=False):\n",
    "\t# querys의 마지막 차원 크기를 가져옴\n",
    "    dim_k = querys.size(-1) # 16\n",
    "    \n",
    "    # 어텐션 스코어 계산, 키 행렬을 전치해서 곱하고, 스케일링 (그레디언트 안정성을 위해서)\n",
    "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    " \n",
    "    # 소프트맥스를 적용\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "\t\n",
    "    # 가중치와 값을 곱해서 최종 어텐션 벡터를 계산\n",
    "    return weights @ values\n",
    "\n",
    "# 어텐션 메커니즘 테스트\n",
    "attention_weights = compute_attention(querys, keys, values)\n",
    "print(\"attention_weights: \", attention_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션 연산을 수행하는 AttentionHead 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3081,  0.0012, -0.1428,  0.2628, -0.2195, -0.1456, -0.3789,\n",
       "           0.3101, -0.6648, -0.0789,  0.2911,  0.5670, -0.1493, -0.1894,\n",
       "           0.1200, -0.0425],\n",
       "         [ 0.3322, -0.0071, -0.1866,  0.1682, -0.3243, -0.2008, -0.4458,\n",
       "           0.1997, -0.6346, -0.0176,  0.2912,  0.4852, -0.0748, -0.0778,\n",
       "           0.1154, -0.1495],\n",
       "         [ 0.3692, -0.0417, -0.1578,  0.1285, -0.2881, -0.1605, -0.3935,\n",
       "           0.1963, -0.7002, -0.0833,  0.2586,  0.5807, -0.1052, -0.1088,\n",
       "           0.0769, -0.1117],\n",
       "         [ 0.3183,  0.0032, -0.1456,  0.2193, -0.2920, -0.1711, -0.4009,\n",
       "           0.2495, -0.6619, -0.0474,  0.2923,  0.5399, -0.1111, -0.1236,\n",
       "           0.1099, -0.0994],\n",
       "         [ 0.3631, -0.0420, -0.2001,  0.1284, -0.2629, -0.1775, -0.4371,\n",
       "           0.2028, -0.6404, -0.0666,  0.2586,  0.5074, -0.0937, -0.1127,\n",
       "           0.1029, -0.1225]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
    "    super().__init__()\n",
    "    self.is_causal = is_causal\n",
    "    self.weight_q = nn.Linear(token_embed_dim, head_dim) # 쿼리 벡터 생성을 위한 선형 층\n",
    "    self.weight_k = nn.Linear(token_embed_dim, head_dim) # 키 벡터 생성을 위한 선형 층\n",
    "    self.weight_v = nn.Linear(token_embed_dim, head_dim) # 값 벡터 생성을 위한 선형 층\n",
    "\n",
    "  def forward(self, querys, keys, values):\n",
    "    outputs = compute_attention(\n",
    "        self.weight_q(querys),  # 쿼리 벡터\n",
    "        self.weight_k(keys),    # 키 벡터\n",
    "        self.weight_v(values),  # 값 벡터\n",
    "        is_causal=self.is_causal\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
    "after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)\n",
    "after_attention_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 멀티 헤드 어텐션 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
    "    super().__init__()\n",
    "    self.n_head = n_head\n",
    "    self.is_causal = is_causal\n",
    "    self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
    "    self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
    "    self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
    "    self.concat_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def forward(self, querys, keys, values):\n",
    "    # B: 배치 크기, T: 시퀀스 길이, C: 임베딩 차원\n",
    "    B, T, C = querys.size()\n",
    "    \n",
    "    # view 로 재구성, 16차원을 4개의 헤드로 4차원식 나눔 \n",
    "    # 멀티 헤드 어텐션에서는 그러면 차원을 여러개로 나눠서 각각의 차원에 대해 가중치 파라미터를 학습하는거임\n",
    "    querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "    keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "    values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "    \n",
    "    # 각 헤드별로 독립적인 어텐션 계산 \n",
    "    attention = compute_attention(querys, keys, values, self.is_causal)\n",
    "    \n",
    "    # 헤드별 어텐션 벡터를 연결하고, 최종 선형 변환\n",
    "    output = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
    "    output = self.concat_linear(output)\n",
    "    return output\n",
    "\n",
    "n_head = 4\n",
    "mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
    "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
    "after_attention_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 층 정규화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.8626e-09,  4.0978e-08,  0.0000e+00,  7.4506e-09,  5.9605e-08]]),\n",
       " tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 파이토치에서는 nn.LayerNorm 을 사용해서 정규화 코드를 만들 수 있음. \n",
    "norm = nn.LayerNorm(embedding_dim)\n",
    "norm_x = norm(input_embeddings)\n",
    "norm_x.shape # torch.Size([1, 5, 16])\n",
    "\n",
    "norm_x.mean(dim=-1).data, norm_x.std(dim=-1).data\n",
    "\n",
    "# (tensor([[ 2.2352e-08, -1.1176e-08, -7.4506e-09, -3.9116e-08, -1.8626e-08]]),\n",
    "#  tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피드 포워드 층 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormFeedForward(nn.Module):\n",
    "  def __init__(self, d_model, dim_feedforward, dropout):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1\n",
    "    self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2\n",
    "    self.dropout1 = nn.Dropout(dropout) # 드랍아웃 층 1\n",
    "    self.dropout2 = nn.Dropout(dropout) # 드랍아웃 층 2\n",
    "    self.activation = nn.GELU() # 활성 함수\n",
    "    self.norm = nn.LayerNorm(d_model) # 층 정규화\n",
    "\n",
    "  def forward(self, src):\n",
    "    x = self.norm(src)\n",
    "    x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "    x = self.dropout2(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 층\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "    super().__init__()\n",
    "    self.attn = MultiheadAttention(d_model, d_model, nhead) # 멀티 헤드 어텐션 클래스\n",
    "    self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n",
    "    self.dropout1 = nn.Dropout(dropout) # 드랍아웃\n",
    "    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) # 피드포워드\n",
    "\n",
    "  def forward(self, src):\n",
    "    norm_x = self.norm1(src)\n",
    "    attn_output = self.attn(norm_x, norm_x, norm_x)\n",
    "    x = src + self.dropout1(attn_output) # 잔차 연결\n",
    "\n",
    "    # 피드 포워드\n",
    "    x = self.feed_forward(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 구현 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_clones(module, N):\n",
    "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, encoder_layer, num_layers):\n",
    "    super().__init__()\n",
    "    self.layers = get_clones(encoder_layer, num_layers)\n",
    "    self.num_layers = num_layers\n",
    "    self.norm = norm\n",
    "\n",
    "  def forward(self, src):\n",
    "    output = src\n",
    "    for mod in self.layers:\n",
    "        output = mod(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더에서 어텐션 연산(마스크 어텐션)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention(querys, keys, values, is_causal=False):\n",
    "\tdim_k = querys.size(-1) # 16\n",
    "\tscores = querys @ keys.transpose(-2, -1) / sqrt(dim_k) # (1, 5, 5)\n",
    "\tif is_causal:\n",
    "\t\tquery_length = querys.size(2)\n",
    "\t\tkey_length = keys.size(2)\n",
    "\t\ttemp_mask = torch.ones(query_length, key_length, dtype=torch.bool).tril(diagonal=0)\n",
    "\t\tscores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n",
    "\tweights = F.softmax(scores, dim=-1) # (1, 5, 5)\n",
    "\treturn weights @ values # (1, 5, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크로스 어텐션이 포함된 디코더 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.self_attn = MultiheadAttention(d_model, d_model, nhead)\n",
    "    self.multihead_attn = MultiheadAttention(d_model, d_model, nhead)\n",
    "    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, tgt, encoder_output, is_causal=True):\n",
    "    # 셀프 어텐션 연산\n",
    "    x = self.norm1(tgt)\n",
    "    x = x + self.dropout1(self.self_attn(x, x, x, is_causal=is_causal))\n",
    "    # 크로스 어텐션 연산\n",
    "    x = self.norm2(x)\n",
    "    x = x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output))\n",
    "    # 피드 포워드 연산\n",
    "    x = self.feed_forward(x)\n",
    "    return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
