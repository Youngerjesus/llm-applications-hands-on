{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 사용량 측정을 위한 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes==0.45.2 (from versions: 0.31.8, 0.32.0, 0.32.1, 0.32.2, 0.32.3, 0.33.0, 0.33.1, 0.34.0, 0.35.0, 0.35.1, 0.35.2, 0.35.3, 0.35.4, 0.36.0, 0.36.0.post1, 0.36.0.post2, 0.37.0, 0.37.1, 0.37.2, 0.38.0, 0.38.0.post1, 0.38.0.post2, 0.38.1, 0.39.0, 0.39.1, 0.40.0, 0.40.0.post1, 0.40.0.post2, 0.40.0.post3, 0.40.0.post4, 0.40.1, 0.40.1.post1, 0.40.2, 0.41.0, 0.41.1, 0.41.2, 0.41.2.post1, 0.41.2.post2, 0.41.3, 0.41.3.post1, 0.41.3.post2, 0.42.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for bitsandbytes==0.45.2\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "런타임 유형을 GPU로 변경하세요\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPU 메모리 사용량: {used_memory:.3f} GB\")\n",
    "    else:\n",
    "        print(\"런타임 유형을 GPU로 변경하세요\")\n",
    "\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/llm-applications-hands-on/llm_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 3 files: 100%|██████████| 3/3 [04:50<00:00, 96.92s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "런타임 유형을 GPU로 변경하세요\n",
      "모델 파라미터 데이터 타입:  torch.float16\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_id) # GPU 메모리 사용량: 2.599 GB\n",
    "print(\"모델 파라미터 데이터 타입: \", model.dtype) # torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "\n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "        lora_config = LoraConfig(\n",
    "                    r=8,\n",
    "                    lora_alpha=32,\n",
    "                    target_modules=[\"query_key_value\"],\n",
    "                    lora_dropout=0.05,\n",
    "                    bias=\"none\",\n",
    "                    task_type=\"CAUSAL_LM\"\n",
    "                )\n",
    "\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def estimate_memory_of_gradients(model):\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_memory += param.grad.nelement() * param.grad.element_size()\n",
    "    return total_memory\n",
    "\n",
    "def estimate_memory_of_optimizer(optimizer):\n",
    "    total_memory = 0\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                total_memory += v.nelement() * v.element_size()\n",
    "    return total_memory\n",
    "\n",
    "def train_model(model, dataset, training_args):\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=training_args.per_device_train_batch_size)\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    model.train()\n",
    "    gpu_utilization_printed = False\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / training_args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if step % training_args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            gradients_memory = estimate_memory_of_gradients(model)\n",
    "            optimizer_memory = estimate_memory_of_optimizer(optimizer)\n",
    "            if not gpu_utilization_printed:\n",
    "                print_gpu_utilization()\n",
    "                gpu_utilization_printed = True\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"옵티마이저 상태의 메모리 사용량: {optimizer_memory / (1024 ** 3):.3f} GB\")\n",
    "    print(f\"그레디언트 메모리 사용량: {gradients_memory / (1024 ** 3):.3f} GB\")\n",
    "\n",
    "\n",
    "def make_dummy_dataset():\n",
    "  seq_len, dataset_size = 256, 64\n",
    "  dummy_data = {\n",
    "      \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "      \"labels\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "  }\n",
    "  dataset = Dataset.from_dict(dummy_data)\n",
    "  dataset.set_format(\"pt\")\n",
    "  return dataset\n",
    "\n",
    "def cleanup():\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "    if 'dataset' in globals():\n",
    "        del globals()['dataset']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def gpu_memory_experiment(batch_size,\n",
    "                          gradient_accumulation_steps=1,\n",
    "                          gradient_checkpointing=False,\n",
    "                          model_id=\"EleutherAI/polyglot-ko-1.3b\",\n",
    "                          peft=None):\n",
    "\n",
    "    print(f\"배치 사이즈: {batch_size}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_id, peft=peft)\n",
    "    if gradient_checkpointing == True or peft == 'qlora':\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    dataset = make_dummy_dataset()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        output_dir=\"./result\",\n",
    "        num_train_epochs=1\n",
    "      )\n",
    "\n",
    "    try:\n",
    "        train_model(model, dataset, training_args)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(e)\n",
    "        else:\n",
    "            raise e\n",
    "    finally:\n",
    "        del model, dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "런타임 유형을 GPU로 변경하세요\n",
      "배치 사이즈: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.1180\n",
      "런타임 유형을 GPU로 변경하세요\n",
      "런타임 유형을 GPU로 변경하세요\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레디언트 메모리 사용량: 0.006 GB\n",
      "런타임 유형을 GPU로 변경하세요\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "gpu_memory_experiment(batch_size=16, peft='lora')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
